{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from data.dataset import NERDataset\n",
    "from models.utils import Namespace, getSignal\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence, pad_sequence, PackedSequence\n",
    "from models.networks import GlobalContextualDeepTransition\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from torch.utils.data import DataLoader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sourceName = 'data/conll03/eng.train.src'\n",
    "targetName = 'data/conll03/eng.train.trg'\n",
    "gloveFile = 'data/conll03/trimmed.300d.Cased.txt'\n",
    "symbFile = 'data/conll03/sym.glove'\n",
    "testSrc = 'data/conll03/eng.testb.src'\n",
    "testTrg = 'data/conll03/eng.testb.trg'\n",
    "\n",
    "data = NERDataset(sourceName, targetName, gloveFile, symbFile)\n",
    "data.readTestFile(testSrc, testTrg)\n",
    "loader = data.getLoader(1024, shuffle=False)\n",
    "# loader = DataLoader(data, collate_fn=data.pack_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prevCheckpointPath = 'lightning_logs/ckpt-epoch=109-train_loss=0.63.ckpt'\n",
    "\n",
    "with open('config.json', 'r') as file:\n",
    "    kwargs = json.load(file)\n",
    "    \n",
    "model = GlobalContextualDeepTransition.load_from_checkpoint(prevCheckpointPath, **kwargs)\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logitsToLogProbs(logits):\n",
    "    return logits - torch.logsumexp(logits, dim=1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    words, chars, charMask, targets = batch = next(iter(loader))\n",
    "    encoded, initHiddenState, initPrevTarget = model.encode(words, chars, charMask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pathSum(values, logProbs):\n",
    "    \"\"\"\n",
    "        Adds the prev sum to current logProbs\n",
    "        to get the effective logprob\n",
    "    \"\"\"\n",
    "    # values is batch, beam\n",
    "    values = torch.unsqueeze(values, -1) # batch, beam, 1\n",
    "    values = values.repeat(1, 1, tags)  # batch, beam, units\n",
    "    values = values.permute(1,0,2)       # beam, batch, units\n",
    "    values = torch.cat(list(values), -1) # batch, units * beam\n",
    "    \n",
    "    # logprobs is [batch, units*beam]\n",
    "    ps = logProbs + values\n",
    "    \n",
    "    # ps is [batch, units*beam]\n",
    "    return ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batchSize=47, beamSize=4, units=256, tags=17\n"
     ]
    }
   ],
   "source": [
    "batchSize = words.batch_sizes[0].item() # batchSize\n",
    "beamSize  = 4 # beamsize\n",
    "units = model.sequenceLabeller.decoderUnits\n",
    "tags = model.numTags\n",
    "\n",
    "print(f\"batchSize={batchSize}, beamSize={beamSize}, units={units}, tags={tags}\")\n",
    "\n",
    "lengths = torch.zeros(batchSize, dtype=torch.int)\n",
    "\n",
    "for x in words.batch_sizes:\n",
    "    lengths[:x] += 1\n",
    "\n",
    "\"\"\"\n",
    "    values[i, j] contains the heuristic beam of the ith example. j in range(beamSize)\n",
    "    paths [i, j] contains the corresponding paths\n",
    "\"\"\"\n",
    "values = torch.zeros(batchSize, beamSize) # we maintain a queue like tensor, each example has a queue of size beamSize\n",
    "paths  = [ [list() for _ in range(beamSize)] for _ in range(batchSize) ] # one node in each queue as root of tree\n",
    "\"\"\"Init nodes in a matrix for each beam and \"\"\"\n",
    "\n",
    "# encoded pages\n",
    "start = 0\n",
    "encodedPages = []\n",
    "for pageLen in words.batch_sizes:\n",
    "    if start == 0:\n",
    "        page = encoded[start:start+pageLen] # first page is not repeated\n",
    "    else:\n",
    "        page = encoded[start:start+pageLen].repeat(beamSize, 1)\n",
    "    encodedPages.append(page) # [e1, e2, e3, e1, e2, e3.. etc, repeated beamSize times]\n",
    "    start += pageLen\n",
    "\n",
    "live = list(range(batchSize))\n",
    "dead = []\n",
    "\n",
    "# initial values are not repeated\n",
    "hiddenState = initHiddenState\n",
    "prevTarget = initPrevTarget\n",
    "\n",
    "\n",
    "for t, b in enumerate(words.batch_sizes):\n",
    "    \"\"\"Get the previous target and make the forward pass\"\"\"\n",
    "    if t == 0:\n",
    "        actualSize = b\n",
    "    else:\n",
    "        actualSize = b * beamSize\n",
    "    prevTarget = prevTarget[:actualSize] + getSignal(1, units, t, model.device)\n",
    "    with torch.no_grad():\n",
    "        hiddenState, logits = model.sequenceLabeller.decode_once(\n",
    "            encodedPages[t],\n",
    "            prevTarget,\n",
    "            hiddenState\n",
    "        )\n",
    "    logProbs = logitsToLogProbs(logits) # [b*beamSize, units] ie [l1, l2, l3, l1, l2, l3 ... numTag d vectors repeated]\n",
    "\n",
    "    \"\"\"Add the logProbs to the current paths to get newPathSums\"\"\"\n",
    "    if t == 0:\n",
    "        ps = logProbs\n",
    "    else:\n",
    "        logProbs = logProbs.reshape((beamSize, b, tags)) # now becomes [[l1, l2, l3], [l1, l2, l3], [l1, l2, l3]...]\n",
    "        logProbs = torch.cat(list(logProbs), dim=-1) # [l1l1l1..., l2l2l2..., l3l3l3...]\n",
    "        ps = pathSum(values[:b], logProbs)\n",
    "\n",
    "    \"\"\"Filter the top beam pathsums and extend the paths\"\"\"\n",
    "    values[:b], indices = ps.topk(dim=-1, k=beamSize) # values is [batch, beam]\n",
    "\n",
    "    # indices represent max over arrays of size units * beam\n",
    "    # Their parent must be at idx/units in the queue.\n",
    "    parents = indices // tags\n",
    "\n",
    "    # the child is the actual index\n",
    "    children = indices % tags\n",
    "\n",
    "    \"\"\"Extend paths using new values\"\"\"\n",
    "    numFinished = 0\n",
    "    for qidx, valBeam, childbeam, parentBeam in zip(live, values, children, parents):\n",
    "        \"\"\"\n",
    "            Narrow our sight to each example:\n",
    "                At qidx, extend the path of parentBeam[i] with childBeam[i]\n",
    "                You will get beam no. of new paths.\n",
    "                This is your new path beam.\n",
    "        \"\"\"\n",
    "        newQueue = []\n",
    "        for v, c, p in zip(valBeam, childbeam, parentBeam):\n",
    "            oldPath = paths[qidx][p]\n",
    "            newPath = oldPath + [c.item()]\n",
    "            newQueue.append(newPath)\n",
    "        paths[qidx] = newQueue\n",
    "\n",
    "        \"\"\"Mark completed if the lenghts of the paths match the word count\"\"\"\n",
    "        if len(newQueue[0]) == lengths[qidx]:\n",
    "            numFinished += 1\n",
    "            dead.append(qidx)\n",
    "    \n",
    "#     print(*paths, sep='\\n')\n",
    "    \"\"\"If an example is done, it has to be at the end of the live array\"\"\"\n",
    "    for _ in range(numFinished):\n",
    "        live.pop()\n",
    "\n",
    "    \"\"\"\n",
    "        Rearrange the prevTarget and hiddenState using indices\n",
    "        Note that values = torch.gather(ps, -1, indices) is a way to go from [batch, units*beam] and [batch, beam]\n",
    "\n",
    "        * ps -> [batch, units*beam] and values -> [batch, beam]\n",
    "        use the same flow as logProbs -> ps -> values \n",
    "    \"\"\"\n",
    "    # remove dead sequences\n",
    "    children = children[:b-numFinished]\n",
    "    parents = parents[:b-numFinished]\n",
    "    prevTarget = model.sequenceLabeller.targetEmbedding(children.T.reshape(-1))\n",
    "    \n",
    "    if t == 0:\n",
    "        hiddenState = torch.unsqueeze(hiddenState, 0)\n",
    "    else:\n",
    "        hiddenState = hiddenState.reshape((beamSize, b, -1))\n",
    "    unfolded = parents.T.reshape(-1)\n",
    "    runner = torch.arange(b-numFinished).repeat(beamSize)\n",
    "    hiddenState = hiddenState[unfolded, runner]\n",
    "\n",
    "#     print(\"Hidden State shape\", hiddenState.shape)\n",
    "#     print(\"prevTarget shape\", prevTarget.shape)\n",
    "# print(*[page.shape for page in encodedPages], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decode results\n",
    "results = [x[0] for x in paths]\n",
    "unsorted = []\n",
    "for i in words.unsorted_indices:\n",
    "    unsorted.append([data.tags[j] for j in results[i]] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'O', 'S-MISC', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'E-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['O', 'O', 'S-MISC', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'E-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "idx = 7\n",
    "actual = [data.tags[j] for j in data[idx][2]] \n",
    "predicted = unsorted[idx]\n",
    "\n",
    "print(actual)\n",
    "print(predicted)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
